from model_restore import *

import torchvision	
import math 
import torch.nn as nn
import torch.nn.functional as F 
import torch

def adapte_model(path1 , path2):    
    pkl_source = path1
    pkl_target = path2
	
    ########################## Source Model : ####################################
    # Because nnU-net creates an optimized model depending of your database, the following model is model generated by longiseg4ms (so by nnU-net) by training a new model on T1+FLAIR over 1 epoch. 
    # With this approach, we are sure that the model created for T1+FLAIR MRIs is optimized following the nnU-net rules
    checkpoint = pkl_source[:-4]
    train = False
    trainer_source = restore_model(pkl_source, checkpoint, train)
    model_source = trainer_source.network.parameters()   
    ##############################################################################
    
    ########################## Target Model : ####################################
    checkpoint = pkl_target[:-4]
    train = False
    trainer_target = restore_model(pkl_target, checkpoint, train)
    model_target = trainer_target.network.parameters()
    ##############################################################################    
    
    
    ############################################################################# Update of the conv_blocks_context #############################################################################
    # On this target model, we get the padding/kernew/stride size for our new conv3D
    conv_blocks_context_target = trainer_target.network.conv_blocks_context
    conv_target = (conv_blocks_context_target[0].blocks[0].conv)
    
    # So now, we can reshape the source model to the target model in order to finetunine this model by just continuing the training.
    conv_blocks_context = trainer_source.network.conv_blocks_context 
    
    ################################## Update of the input of the encoder
    # With the following part, we just double the number of input chanels by duplicating the weights of the first chanel
    p = next(conv_blocks_context.parameters())
    conv1_type = p.dtype
    conv1_weight = p.float()

    in_chans = 1  ### or 2 T1+FLAIR
    repeat = int(math.ceil(in_chans/1))    
    conv1_weight = conv1_weight.repeat(1 , repeat , 1 , 1 , 1 )[:,:in_chans,:,:,:,]
    conv1_weight *= (1 / float(in_chans))
    conv1_weight = conv1_weight.to(conv1_type)
    #updated_model = torch.nn.Conv3d(2, 32, kernel_size=conv_target.kernel_size, stride= conv_target.stride, padding= conv_target.padding) #T1+FLAIR
    updated_model = torch.nn.Conv3d(1, 32, kernel_size=conv_target.kernel_size, stride=conv_target.stride, padding=conv_target.padding)
    updated_model.weight = torch.nn.Parameter(conv1_weight)
    trainer_source.network.conv_blocks_context[0].blocks[0].conv = updated_model
    
    # Now, we adapte the shape the source model so that it matches with the target one 
    conv_layer = conv_blocks_context[0].blocks[0].conv
    tensor = conv_layer.weight.data

    # Now, we have to compare the size of the conv layers because they can be different and if so we have to squeeze the source one to match the target one
    if not ( conv_layer.weight.data.size() == conv_target.weight.data.size() ):
        if ((conv_layer.weight.data.size()) > (conv_target.weight.data.size()) ): # If the source is too big, we squeeze it
            pr = list(conv_layer.weight.data.shape)
            pr_b = list(conv_target.weight.data.shape)
            for i in range(len(pr)):
                while(pr[i] != pr_b[i]):
                    if(i==2):
                        conv_layer.weight.data = conv_layer.weight.data[:, :, :-1, :, :]
                    if(i==3):
                        conv_layer.weight.data = conv_layer.weight.data[:, :, :, :-1, :]
                    if(i==4):
                        conv_layer.weight.data = conv_layer.weight.data[:, :, :, :, :-1]     
                    pr = list(conv_layer.weight.data.shape)
                    pr_b = list(conv_target.weight.data.shape)                                           
        else :
            print(" THIS SHOULD NOT HAPPEN ")            


    
    if(conv_layer.bias.numel() > conv_target.bias.numel() ): # If the bias vector size is too large, the former are kept so that the size matches that of the target 
    	conv_blocks_context[0].blocks[0].conv.bias = nn.Parameter( conv_blocks_context[0].blocks[0].conv.bias[:conv_target.bias.numel()] , requires_grad=True) 
    	
    elif(conv_layer.bias.numel() < conv_target.bias.numel() ): # If the bias vector size is too small, we add random biases
        new_bias = torch.cat(( conv_blocks_context[0].blocks[0].conv.bias[:] , torch.randn(conv_target.bias.numel() - conv_layer.bias.numel())  ))
        conv_blocks_context[0].blocks[0].conv.bias = nn.Parameter( new_bias , requires_grad=True) 





    
    ################################## Update of the output of the encoder
    it_source = conv_blocks_context[5][0].blocks[0].conv
    it_target = conv_blocks_context_target[5][0].blocks[0].conv

    # like before, we have to check that everything is ok :
    # Size update
    if not ( it_source.weight.data.size() == it_target.weight.data.size() ):
        if (torch.numel(it_source.weight.data) > torch.numel(it_target.weight.data) ):
            while( torch.numel(it_source.weight.data) > torch.numel(it_target.weight.data) ):
                it_source.weight.data = torch.squeeze(it_source.weight.data)
        else:
             print("THIS SHOULD NOT HAPPEN")

    # Bias update
    if(it_source.bias.numel() > it_target.bias.numel() ): 
    	conv_blocks_context[5][0].blocks[0].conv.bias = nn.Parameter( conv_blocks_context_target[5][0].blocks[0].conv.bias[:it_target.bias.numel()] , requires_grad=True) 
    elif(it_source.bias.numel() < it_target.bias.numel() ):
        new_bias = torch.cat(( conv_blocks_context[5][0].blocks[0].conv.bias[:] , torch.randn(it_target.bias.numel() - it_source.bias.numel())  ))
        conv_blocks_context[5][0].blocks[0].conv.bias = nn.Parameter( new_bias , requires_grad=True) 


    it_source.padding = it_target.padding  
    it_source.kernel_size = it_target.kernel_size  
    it_source.stride = it_target.stride




    ############################################################################# Update of the (tu) Module List : #############################################################################
     
    tu_source = trainer_source.network.tu[0]
    tu_target = trainer_target.network.tu[0]
    e =  tu_source

    tensor = e.weight.data

    model_new = nn.ConvTranspose3d(320, 320, kernel_size=tu_target.kernel_size, stride=tu_target.stride, bias=False) # Here we just put the correct kernel qnd stride size
    model_new.weight.data = tensor
    trainer_source.network.tu[0] = model_new
    
    cpt = 0
    if not ( trainer_source.network.tu[0].weight.data.size() == trainer_target.network.tu[0].weight.data.size() ):
        if (trainer_source.network.tu[0].weight.data.size() > trainer_target.network.tu[0].weight.data.size() ): # If the source is too big, we squeeze it
            pr = list(trainer_source.network.tu[0].weight.data.shape)
            pr_b = list(trainer_target.network.tu[0].weight.data.shape)
            for i in range(len(pr)):

                while(pr[i] != pr_b[i]):
                    if(i==2):
                        trainer_source.network.tu[0].weight.data = trainer_source.network.tu[0].weight.data[:, :, :-1, :, :]
                    if(i==3):
                        trainer_source.network.tu[0].weight.data = trainer_source.network.tu[0].weight.data[:, :, :, :-1, :]
                    if(i==4):
                        trainer_source.network.tu[0].weight.data = trainer_source.network.tu[0].weight.data[:, :, :, :, :-1] 
                    pr = list(trainer_source.network.tu[0].weight.data.shape)
                    pr_b = list(trainer_target.network.tu[0].weight.data.shape)                                             
        else :
            print(" THIS SHOULD NOT HAPPEN ")            

    trainer_source.epoch = 0
    trainer_target.epoch = 0


    trainer_source.save_checkpoint("model_final_checkpoint.model")


if __name__ == "__main__":
    #path1 = "/home/francy/Desktop/Projet_LounesMeddahi/longiseg4ms/nnunet_data/models/nnUNet/3d_fullres/Task700_ModelV2/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model.pkl"
    path1="/home/francy/Desktop/FG_AVC_2023/longiseg4ms/nnunet_data/models/nnUNet/3d_fullres/Task111/nnUNetTrainerV2__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model.pkl"
    #path2 = "/home/francy/Desktop/Projet_LounesMeddahi/longiseg4ms/nnunet_data/models/nnUNet/3d_fullres/Task444_Model/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model.pkl"
    path2="/home/francy/Desktop/FG_AVC_2023/longiseg4ms/nnunet_data/models/nnUNet/3d_fullres/Task999/nnUNetTrainerV2__nnUNetPlansv2.1/fold_4/model_final_checkpoint.model.pkl"

    adapte_model(path1 , path2)
    
    
    

	
